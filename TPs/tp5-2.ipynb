{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 5.2 - Vision et language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom='jai'\n",
    "prenom='ilyass'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 : Simplification du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import _pickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "filename = 'flickr_8k_train_dataset.txt'\n",
    "df = pd.read_csv(filename, delimiter='\\t')\n",
    "nb_samples = df.shape[0]\n",
    "iter = df.iterrows()\n",
    "\n",
    "bow = {}\n",
    "nbwords = 0\n",
    "\n",
    "for i in range(nb_samples):\n",
    " x = iter.__next__()\n",
    " cap_words = x[1][1].split() # split caption into words\n",
    " cap_wordsl = [w.lower() for w in cap_words] # remove capital letters\n",
    " nbwords += len(cap_wordsl)\n",
    " for w in cap_wordsl:\n",
    "  if (w in bow):\n",
    "   bow[w] = bow[w]+1\n",
    "  else:\n",
    "   bow[w] = 1\n",
    "\n",
    "bown = sorted([(value,key) for (key,value) in bow.items()], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbkeep = 1000 # 100 is needed for fast processing\n",
    "freqnc = np.cumsum([float(w[0])/nbwords*100.0 for w in bown])\n",
    "print(\"number of kept words=\"+str(nbkeep)+\" - ratio=\"+str(freqnc[nbkeep-1])+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [str(bown[i][1]) for i in range(100)]\n",
    "plt.figure(dpi=300)\n",
    "plt.xticks(rotation=90, fontsize=3)\n",
    "plt.ylabel('Word Frequency')\n",
    "plt.bar(x_axis, freqnc[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbkeep = 100 # 100 is needed for fast processing\n",
    "\n",
    "outfile = 'Caption_Embeddings.p'\n",
    "[listwords, embeddings] = pickle.load( open( outfile, \"rb\" ) )\n",
    "\n",
    "embeddings_new = np.zeros((nbkeep,102))\n",
    "listwords_new = []\n",
    "\n",
    "for i in range(nbkeep):\n",
    " listwords_new.append(bown[i][1])\n",
    " embeddings_new[i,:] = embeddings[listwords.index(bown[i][1]), :]\n",
    " embeddings_new[i,:] /= np.linalg.norm(embeddings_new[i,:]) # Normalization\n",
    "\n",
    "\n",
    "listwords = listwords_new\n",
    "embeddings = embeddings_new\n",
    "outfile = \"Caption_Embeddings_\"+str(nbkeep)+\".p\"\n",
    "with open(outfile, \"wb\" ) as pickle_f:\n",
    " pickle.dump( [listwords, embeddings], pickle_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplification du vocabulaire\n",
    "\n",
    "La simplification du vocabulaire est le processus de réduction de la taille et de la complexité d’un vocabulaire, souvent en supprimant des mots rares ou peu utilisés et en les remplaçant par des alternatives plus simples et fréquemment utilisées. Cela peut améliorer la lisibilité, la compréhension et l’accessibilité globale d’un langage écrit ou parlé.\n",
    "\n",
    "Dans cette première partie du TP, nous allons extraire un histogramme d’occurrences de mots à partir des légendes du sous-ensemble d’entraînement de Flickr8k. Pour accélérer le temps d’entraînement du modèle, nous allons utiliser un sous-ensemble du vocabulaire provenant de l’intégration textuelle du TP précédent.\n",
    "\n",
    "Pour ce faire :\n",
    "1. Nous avons d'abord chargé les mots des légendes du sous-ensemble d’entraînement de Flickr8k, puis les avons triés selon leur fréquence d’occurrence.\n",
    "2. Ensuite, nous avons chargé le fichier d’intégration du TP précédent et conservé les 1000 mots les plus fréquents. Nous avons ensuite sauvegardé ce sous-ensemble de mots ainsi que les vecteurs d’intégration Glove correspondants.\n",
    "3. Enfin, nous avons calculé la fréquence cumulée des 100 premiers mots conservés.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : Création des données d’apprentissage et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'flickr_8k_train_dataset.txt'\n",
    "df = pd.read_csv(filename, delimiter='\\t')\n",
    "nbTrain = df.shape[0]\n",
    "iter_w = df.iterrows()\n",
    "\n",
    "\n",
    "# Legends\n",
    "caps = []\n",
    "\n",
    "# Images \n",
    "imgs = []\n",
    "for i in range(nbTrain):\n",
    "    x = iter_w.__next__()\n",
    "    caps.append(x[1][1])\n",
    "    imgs.append(x[1][0])\n",
    "\n",
    "maxLCap = 0\n",
    "\n",
    "for caption in caps:\n",
    "    l = 0\n",
    "    words_in_caption = caption.split()\n",
    "    for j in range(len(words_in_caption) - 1):\n",
    "        current_w = words_in_caption[j].lower()\n",
    "        if current_w in listwords:\n",
    "            l += 1\n",
    "        if l > maxLCap:\n",
    "            maxLCap = l\n",
    "\n",
    "print(\"max caption length =\" + str(maxLCap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get  # to make GET request\n",
    "\n",
    "def download(url, file_name):\n",
    "    # open in binary mode\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        # get request\n",
    "        response = get(url)\n",
    "        # write to file\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features \n",
    "download('http://cedric.cnam.fr/~thomen/cours/US330X/encoded_images_PCA.p', \"encoded_images_PCA.p\")\n",
    "encoded_images = pickle.load(open(\"encoded_images_PCA.p\", \"rb\"))\n",
    "\n",
    "indexwords = {}\n",
    "for i in range(len(listwords)):\n",
    "    indexwords[listwords[i]] = i\n",
    "\n",
    "tinput = 202\n",
    "\n",
    "tVocabulary = len(listwords)\n",
    "\n",
    "X_train = np.zeros((nbTrain, maxLCap, tinput))\n",
    "Y_train = np.zeros((nbTrain, maxLCap, tVocabulary), bool)\n",
    "\n",
    "ll = 50\n",
    "nbtot = 0\n",
    "nbkept = 0\n",
    "\n",
    "for i in range(nbTrain):\n",
    "    words_in_caption = caps[i].split()\n",
    "\n",
    "    nbtot += len(words_in_caption) - 1\n",
    "    indseq = 0\n",
    "    for j in range(len(words_in_caption) - 1):\n",
    "\n",
    "        current_w = words_in_caption[j].lower()\n",
    "\n",
    "        if j == 0 and current_w != '<start>':\n",
    "            print(\"PROBLEM\")\n",
    "        if current_w in listwords:\n",
    "            X_train[i, indseq, 0:100] = encoded_images[imgs[i]]\n",
    "            X_train[i, indseq, 100:202] = embeddings[listwords.index(current_w), :]\n",
    "\n",
    "        next_w = words_in_caption[j + 1].lower()\n",
    "\n",
    "        index_pred = 0\n",
    "        if next_w in listwords:\n",
    "            nbkept += 1\n",
    "            index_pred = indexwords[next_w]\n",
    "            Y_train[i, indseq, index_pred] = True\n",
    "\n",
    "            indseq += 1\n",
    "\n",
    "outfile = 'Training_data_' + str(nbkeep)\n",
    "\n",
    "\n",
    "np.savez(outfile, X_train=X_train, Y_train=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "download('http://cedric.cnam.fr/~thomen/cours/US330X/flickr_8k_test_dataset.txt', \"flickr_8k_test_dataset.txt\")\n",
    "filename = 'flickr_8k_test_dataset.txt'\n",
    "df = pd.read_csv(filename, delimiter='\\t')\n",
    "nbTest = df.shape[0]\n",
    "iter_w = df.iterrows()\n",
    "\n",
    "\n",
    "# Legends\n",
    "caps = []\n",
    "# Images\n",
    "imgs = []\n",
    "for i in range(nbTest):\n",
    "    x = iter_w.__next__()\n",
    "    caps.append(x[1][1])\n",
    "    imgs.append(x[1][0])\n",
    "\n",
    "indexwords = {}\n",
    "for i in range(len(listwords)):\n",
    "    indexwords[listwords[i]] = i\n",
    "\n",
    "# Features \n",
    "encoded_images = pickle.load(open(\"encoded_images_PCA.p\", \"rb\"))\n",
    "\n",
    "tVocabulary = len(listwords)\n",
    "X_test = np.zeros((nbTest, maxLCap, tinput))\n",
    "Y_test = np.zeros((nbTest, maxLCap, tVocabulary), bool)\n",
    "\n",
    "for i in range(nbTest):\n",
    "    words_in_caption = caps[i].split()\n",
    "    indseq = 0\n",
    "    for j in range(len(words_in_caption) - 1):\n",
    "        current_w = words_in_caption[j].lower()\n",
    "        if current_w in listwords:\n",
    "            X_test[i, indseq, 0:100] = encoded_images[imgs[i]]\n",
    "            X_test[i, indseq, 100:202] = embeddings[listwords.index(current_w), :]\n",
    "\n",
    "        next_w = words_in_caption[j + 1].lower()\n",
    "        if next_w in listwords:\n",
    "            index_pred = indexwords[next_w]\n",
    "            Y_test[i, indseq, index_pred] = True\n",
    "            indseq += 1\n",
    "\n",
    "outfile = 'Test_data_' + str(nbkeep)\n",
    "np.savez(outfile, X_test=X_test, Y_test=Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stockage des données d’entraînement\n",
    "\n",
    "Nous allons stocker nos données d’entraînement, c’est-à-dire les tenseurs contenant les données et les étiquettes. Le tenseur de données **X** aura une taille de **Ns×Ls×d**, où :\n",
    "\n",
    "- **Ns** représente le nombre de séquences (légendes),\n",
    "- **Ls** est la longueur des séquences,\n",
    "- **d** est la taille du vecteur décrivant chaque mot de la séquence.\n",
    "\n",
    "Nous avons construit les tenseurs de données et d’étiquettes pour les données d’entraînement ainsi que pour les données qui seront utilisées pour tester notre modèle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3 : Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, filename, batch_size):\n",
    "        self.filename = filename\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def generator(self):\n",
    "        while True:\n",
    "            data = np.load(self.filename)\n",
    "            total_samples = len(data['X_train'])\n",
    "            indexes = np.arange(total_samples)\n",
    "            np.random.shuffle(indexes)\n",
    "            \n",
    "            for start in range(0, total_samples, self.batch_size):\n",
    "                end = min(start + self.batch_size, total_samples)\n",
    "                batch_indexes = indexes[start:end]\n",
    "                X_batch = data['X_train'][batch_indexes]\n",
    "                y_batch = data['Y_train'][batch_indexes]\n",
    "                yield X_batch, y_batch\n",
    "\n",
    "# Utilisation :\n",
    "train_generator = DataGenerator('Training_data_100.npz', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Masking\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Creation of the model\n",
    "SEQLEN = 35\n",
    "taille_chars = 202\n",
    "HSIZE = 100\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0, input_shape=(SEQLEN, taille_chars)))\n",
    "model.add(SimpleRNN(HSIZE, return_sequences=True, input_shape=(SEQLEN, taille_chars), unroll=True))\n",
    "model.add(Dense(1000, name='fc1'))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.summary()\n",
    "nbkeep = 1000\n",
    "\n",
    "# Load train data\n",
    "# Load train data TRAIN GENERATOR\n",
    "\n",
    "\n",
    "\n",
    "# Load test data\n",
    "Test_data = np.load('Test_data_' + str(nbkeep) + '.npz')\n",
    "X_test = Test_data['X_test']\n",
    "Y_test = Test_data['Y_test']\n",
    "\n",
    "# Compiling\n",
    "BATCH_SIZE = 10\n",
    "NUM_EPOCHS = 10\n",
    "optim = Adam()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optim, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS)\n",
    "\n",
    "# Evaluation\n",
    "scores_train = model.evaluate(X_train, Y_train, verbose=1)\n",
    "scores_test = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"PERFS TRAIN: %s: %.2f%%\" % (model.metrics_names[1], scores_train[1] * 100))\n",
    "print(\"PERFS TEST: %s: %.2f%%\" % (model.metrics_names[1], scores_test[1] * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, savename):\n",
    "    model_json = model.to_json()\n",
    "    with open(savename + \".json\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model_json)\n",
    "    print(\"json Model \", savename, \".json saved to disk\")\n",
    "    model.save_weights(savename + \".h5\")\n",
    "    print(\"Weights \", savename, \".h5 saved to disk\")\n",
    "\n",
    "\n",
    "def load_model(savename):\n",
    "    with open(savename + \".json\", \"r\") as yaml_file:\n",
    "        model = model_from_json(yaml_file.read())\n",
    "    print(\"json Model \", savename, \".json loaded \")\n",
    "    model.load_weights(savename + \".h5\")\n",
    "    print(\"Weights \", savename, \".h5 loaded \")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameModel = 'vision'\n",
    "save_model(model, nameModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCICE 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    predsN = pow(preds, 1.0 / temperature)\n",
    "    predsN /= np.sum(predsN)\n",
    "    probas = np.random.multinomial(1, predsN, 1)\n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import _pickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Load Model\n",
    "nameModel = 'vision'\n",
    "model = load_model(nameModel)\n",
    "\n",
    "# Compilation \n",
    "optim = Adam()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optim, metrics=['accuracy'])\n",
    "\n",
    "# Load Test data\n",
    "nbkeep = 1000\n",
    "outfile = 'Test_data_' + str(nbkeep) + '.npz'\n",
    "Test_data = np.load(outfile)\n",
    "X_test = Test_data['X_test']\n",
    "Y_test = Test_data['Y_test']\n",
    "\n",
    "outfile = \"Caption_Embeddings_\" + str(nbkeep) + \".p\"\n",
    "[listwords, embeddings] = pickle.load(open(outfile, \"rb\"))\n",
    "indexwords = {}\n",
    "for i in range(len(listwords)):\n",
    "    indexwords[listwords[i]] = i\n",
    "\n",
    "# Display one image\n",
    "ind = np.random.randint(X_test.shape[0])\n",
    "filename = 'flickr_8k_test_dataset.txt'\n",
    "df = pd.read_csv(filename, delimiter='\\t')\n",
    "iter_w = df.iterrows()\n",
    "\n",
    "for i in range(ind + 1):\n",
    "    x = iter_w.__next__()\n",
    "\n",
    "imname = x[1][0]\n",
    "print(\"image name=\" + imname + \" caption=\" + x[1][1])\n",
    "dirIm = \"./Flicker8k_Dataset/\"\n",
    "\n",
    "img = mpimg.imread(dirIm + imname)\n",
    "plt.figure(dpi=100)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Prediction\n",
    "pred = model.predict(X_test[ind:ind + 1, :, :])\n",
    "\n",
    "\n",
    "nbGen = 5\n",
    "temperature = 0.1\n",
    "for s in range(nbGen):\n",
    "    wordpreds = \"Caption n° \" + str(s + 1) + \": \"\n",
    "    indpred = sampling(pred[0, 0, :], temperature)\n",
    "    wordpred = listwords[indpred]\n",
    "    wordpreds += str(wordpred) + \" \"\n",
    "    X_test[ind:ind + 1, 1, 100:202] = embeddings[indpred]\n",
    "    cpt = 1\n",
    "    while str(wordpred) != '<end>' and cpt < 30:\n",
    "        pred = model.predict(X_test[ind:ind + 1, :, :])\n",
    "        indpred = sampling(pred[0, cpt, :], temperature)\n",
    "        wordpred = listwords[indpred]\n",
    "        wordpreds += str(wordpred) + \" \"\n",
    "        cpt += 1\n",
    "        X_test[ind:ind + 1, cpt, 100:202] = embeddings[indpred]\n",
    "\n",
    "    print(wordpreds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous entraînons notre modèle. Pour cela, nous devons définir son architecture.  \n",
    "Le modèle sera de type séquentiel : il contiendra d’abord une couche de **masque** qui ne calculera pas l’erreur pour les positions où il n’y a pas de mot dans la séquence d’entrée. Ensuite, il inclura une couche de réseau récurrent constitué de **100 neurones** de type *SimpleRNN*. Enfin, nous aurons une couche entièrement connectée, suivie d’une **fonction d’activation softmax**.\n",
    "\n",
    "La fonction de coût choisie est la **cross-entropy**, et nous utilisons l’optimiseur **Adam** en conservant son pas de gradient par défaut. Nous adoptons une **taille de batch** (batch size) de 10.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "import _pickle as pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "\n",
    "nbkeep = 1000\n",
    "outfile = 'Test_data_' + str(nbkeep) + '.npz'\n",
    "npzfile = np.load(outfile)\n",
    "X_test = npzfile['X_test']\n",
    "Y_test = npzfile['Y_test']\n",
    "\n",
    "nameModel = 'model_exo5'\n",
    "model = load_model(nameModel)\n",
    "\n",
    "optim = Adam()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optim, metrics=['accuracy'])\n",
    "scores_test = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"PERFS TEST: %s: %.2f%%\" % (model.metrics_names[1], scores_test[1] * 100))\n",
    "\n",
    "outfile = \"Caption_Embeddings_\" + str(nbkeep) + \".p\"\n",
    "[listwords, embeddings] = pickle.load(open(outfile, \"rb\"))\n",
    "indexwords = {}\n",
    "for i in range(len(listwords)):\n",
    "    indexwords[listwords[i]] = i\n",
    "\n",
    "predictions = []\n",
    "nbTest = X_test.shape[0]\n",
    "for i in range(0, nbTest, 5):\n",
    "    pred = model.predict(X_test[i:i + 1, :, :])\n",
    "    wordpreds = []\n",
    "    indpred = np.argmax(pred[0, 0, :])\n",
    "    wordpred = listwords[indpred]\n",
    "    wordpreds.append(str(wordpred))\n",
    "    X_test[i, 1, 100:202] = embeddings[indpred]\n",
    "    cpt = 1\n",
    "    while str(wordpred) != '<end>' and cpt < (X_test.shape[1] - 1):\n",
    "        pred = model.predict(X_test[i:i + 1, :, :])\n",
    "        indpred = np.argmax(pred[0, cpt, :])\n",
    "        wordpred = listwords[indpred]\n",
    "        if wordpred != '<end>':\n",
    "            wordpreds.append(str(wordpred))\n",
    "            cpt += 1\n",
    "        X_test[i, cpt, 100:202] = embeddings[indpred]\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(\"i=\" + str(i) + \" \" + str(wordpreds))\n",
    "\n",
    "    predictions.append(wordpreds)\n",
    "\n",
    "references = []\n",
    "filename = 'flickr_8k_test_dataset.txt'\n",
    "df = pd.read_csv(filename, delimiter='\\t')\n",
    "iter_w = df.iterrows()\n",
    "\n",
    "ccpt = 0\n",
    "for i in range(nbTest // 5):\n",
    "    captions_image = []\n",
    "    for j in range(5):\n",
    "        x = iter_w.__next__()\n",
    "        ll = x[1][1].split()\n",
    "        caption = []\n",
    "        for k in range(1, len(ll) - 1):\n",
    "            caption.append(ll[k])\n",
    "\n",
    "    captions_image.append(caption)\n",
    "    ccpt += 1\n",
    "\n",
    "    references.append(captions_image)\n",
    "\n",
    "# BLUE-1, BLUE-2, BLUE-3, BLUE-4\n",
    "blue_scores = np.zeros(4)\n",
    "weights = np.zeros((4, 4))\n",
    "weights[0, 0] = 1\n",
    "weights[1, 0] = 0.5\n",
    "weights[1, 1] = 0.5\n",
    "weights[2, 0] = 1.0 / 3.0\n",
    "weights[2, 1] = 1.0 / 3.0\n",
    "weights[2, 2] = 1.0 / 3.0\n",
    "weights[3, :] = 1.0 / 4.0\n",
    "\n",
    "for i in range(4):\n",
    "    blue_scores[i] = nltk.translate.bleu_score.corpus_bleu(references, predictions, weights=(\n",
    "        weights[i, 0], weights[i, 1], weights[i, 2], weights[i, 3]))\n",
    "    print(\"blue_score - \" + str(i) + \"=\" + str(blue_scores[i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
